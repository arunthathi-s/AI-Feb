# -*- coding: utf-8 -*-
"""Text_Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4tu6_YKLyCzo4rxfJcVxCMgZ0wo0oFs
"""

pip install np_utils

import numpy
import sys
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint
import np_utils
from keras.callbacks import ModelCheckpoint

import tensorflow as tf
import numpy as np
import os
import time

# Load a text dataset (For example, using a sample text file)
path_to_file = 'textgen.txt'

# Read and preprocess the text
with open(path_to_file, 'r') as f:
    text = f.read()

# Create a unique mapping of characters to integers and vice versa
vocab = sorted(set(text))
char_to_idx = {char: idx for idx, char in enumerate(vocab)}
idx_to_char = np.array(vocab)

# Convert the text to a list of integers
text_as_int = np.array([char_to_idx[c] for c in text])

# Define the sequence length and the number of sequences
seq_length = 100
examples_per_epoch = len(text) // seq_length

# Create sequences of the given length
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

# Split into input (X) and target (y) sequences
def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)

# Batch the dataset and shuffle it
BATCH_SIZE = 64
BUFFER_SIZE = 10000
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# Build the model: LSTM network
import tensorflow as tf
import numpy as np
import os
import time

# ... (rest of your code remains the same) ...

# Build the model: LSTM network
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(vocab), 256), # Remove batch_input_shape here
    tf.keras.layers.LSTM(1024, return_sequences=True, stateful=True),
    tf.keras.layers.Dense(len(vocab))
])

# Build the model with the input shape
model.build(input_shape=(BATCH_SIZE, None)) # Define the input shape here


# Compile the model
model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))

# ... (rest of your code remains the same) ...
# Compile the model
model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))

# Train the model
EPOCHS = 5
model.fit(dataset, epochs=EPOCHS)

# Function to generate text based on a starting string
def generate_text(model, start_string, num_generate=500):
    input_eval = [char_to_idx[s] for s in start_string]
    input_eval = tf.convert_to_tensor(input_eval)
    input_eval = input_eval[tf.newaxis, :]

    generated_text = []

    # Reset the states of the LSTM layer, not the entire model
    for layer in model.layers:
        if isinstance(layer, tf.keras.layers.LSTM):
            layer.reset_states()

    for _ in range(num_generate):
        predictions = model(input_eval)
        predictions = predictions[:, -1, :]
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()

        generated_text.append(idx_to_char[predicted_id])

        input_eval = tf.concat([input_eval[:, 1:], [[predicted_id]]], axis=-1)

    return start_string + ''.join(generated_text)


# Example usage
start_string = "I imagined that I also might obtain a niche"
generated_text = generate_text(model, start_string)
print(generated_text)









